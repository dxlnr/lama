import tinygrad


def attention_layer(q, k, v, mask=None):
    """"Scaled Dot-Product Attention."""
    pass
    # q = tinygrad.nn.Linear(64, 64)
    # k = tinygrad.nn.Linear(64, 64)
    # v = tinygrad.nn.Linear(64, 64)


def transformer():
    """."""
    pass


def main():
    """."""
    pass


if __name__ == "__main__":
    main() 
